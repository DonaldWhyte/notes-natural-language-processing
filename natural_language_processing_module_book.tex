%
% Name: Natural Language Processing
% Author: Donald Whyte (sc10dw@leeds.ac.uk)
%

\documentclass{article}

\usepackage[margin=2cm]{geometry} % easy page formatting
	\geometry{letterpaper}
\usepackage{doc} %special logo commands
\usepackage{url} % formatting URLs
\usepackage{datetime} % up-to-date, automatically generated times
% For graphic files
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{float}
\usepackage{amsmath}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% Set the title, author, and date.
\title{Natural Language Processing \\ COMP3310 -- AI32}
\author{Donald Whyte}
\date{\today}

% The document proper.
\begin{document}

% Add the title section.
\maketitle

% Add various lists on new pages.
\tableofcontents

\pagebreak
\listoffigures

\pagebreak
\listoftables

% Start the paper on a new page.
\pagebreak

\section{Words}

\subsection{Corpora}

A \textbf{corpus} is a finite body of naturally occurring text that is \textit{not} automatically generated or written specifically for use in NLP. The text used is normally selected according to criteria derived from the needs of the NLP application.

What corpus you use changes what you learn, so it is important to be careful and choose data representative of the problem you're trying to solve. For example, well-edited text (e.g. news articles) is better for deriving grammatical structure. Facebook conversations (say) would have a lot of incorrect spelling and grammar; these errors are simply noise that make it harder for AI to derive grammar.

Properties of corpora:
\begin{itemize}
	\item \textbf{Language Type} -- is it edit text, spontaneous speech? Is it written following standards or are there dialects present?
	\item \textbf{Genre and Domain} -- is it 18th century novels, newspaper text, train enquiry dialogue, FB conversations, etc.?
	\item \textbf{Media} -- text, audio or video?
	\item \textbf{Size} -- how large is the corpus. Bigger almost always means better!
\end{itemize}

A \textbf{balanced corpus} is one which tries to be representative across an entire language or domain. It contains many different tones of text from a language, instead of specifying a single area (not just news or FB conversations -- all kinds of text!)

Common corpora:
\begin{itemize}
	\item \textbf{Brown} -- Famous early corporate of 1 million words. Part-of-speech tagged. Balanced corpus of written American English.
	\item \textbf{LOB} -- Lancaster-Oslo/Bergen corpus. Published British English text.
	\item \textbf{Web 1T Corpus (2006)} -- 1 trillion words of text grabbed from the web, with many domains and languages.
	\item \textbf{Penn Treebank} -- \textbf{parsed}, not just tokenised, text of 2 million words. Domain is newswire and language is American English.
\end{itemize}

\subsection{Tokenisation}

\subsubsection{Definitions}

\textbf{Tokenisation} is a \textbf{processing step} where the input text is automatically divided into atomic units called tokens. A \textbf{token} is either a word, number, or a punctuation mark. \textbf{Word} throughout this document means:
\begin{quote}
	\textit{"continuous alphanumeric characters, delineated by whitespace"}
\end{quote}
where \textbf{whitespace} is spaces, tabs and newlines.

A token is an \textit{individual occurrence} of a word. A textbf{token type} is the word itself without context. For example, the sentence "what is the big elephant and what makes it big?" has 11 tokens (including the question mark) and 9 token types (what and big appear twice).

Delimiting by whitespace may not be enough however. Consider the case below:
Here "data base" carries the atomic meaning but will be considered \textit{two} tokens. Additionally, "it's," contains a \textit{trailing comma}. This is not desired because it makes it look like a different token type to "it's", so it may look like it has a different meaning (when it doesn't). Using \textbf{regular expressions} for tokenising may be more suitable, as it can deal with such cases (especially trailing punctuation).

\textbf{Lexical diversity} (see Equation) \ref{eq:lexical-diversity} measures how diverse the vocabulary of a body of a text is. This measurement is the \textbf{average} number of times a token type occurs in the text. Therefore, if the same words are used often there will be \textbf{high} lexical diversity. Likewise, if there is large variety of words, then lexical diversity will be \textbf{low}.

\begin{equation}
	\frac{numTokens}{numTokenTypes}
	\label{eq:lexical-diversity}
\end{equation}

\subsubsection{Issues}

\begin{itemize}
	\item \textbf{Sentence Boundaries} -- e.g. punctuation, quotation marks around sentences? What does a sentence begin/end? For example, "I said 'Bye Joe.' and laughed!". This a whole sentence, but a tokeniser might think "Joe." is the end of the sentence due to the full stop.
	\item \textbf{Proper Names} -- proper names are often composed of multiple words delimited by spaces, such as "Donald Whyte" or "New York". How do we detect that they belong together as a single unit?
	\item \textbf{Contractions/Possession} -- when is an apostrophe used for contractions or possession? "That’s Fred’s jacket’s pocket." should jacket's be expanded to "jacket is" just like that's to "that is"? What makes "jacket's" possession but not "that's"? How commonly it's usedas a contraction in labelled corpora?
\end{itemize}

\subsection{Morphology}

\textbf{Morphology} is the study of the way words are built up from \textbf{smaller meaning units}. \textbf{Morphemes} are the \textit{smallest meaningful unit} in the grammar of a language. A word is composed of one or more morphemes.

Morpheme definitions:
\begin{itemize}
	\item \textbf{Root} -- portion of word that is \textbf{common} to a set of derived or inflected forms when all \textbf{affixes are removed}. The root cannot be broken down into further analysable, meaning elements and it carries the \textbf{principle portion} of the meaning of the words.
	\item \textbf{Stem} -- The root(s) of a word with \textbf{derivational affixes} included, but \textbf{inflectional affixes removed} This form is ready for inflectional affixes to be added, but the stem does not contain these.
	\item \textbf{Affix} -- a \textbf{bound} morpheme that cannot be used by itself. It must be joined \textbf{before, after} or \textbf{within} a root or stem. There are four types of affixes:
	\begin{itemize}
		\item \textbf{Prefixes} -- before stem, such as \textbf{antidis}establishmentarianism
		\item \textbf{Suffixes} -- after stem, such as antidisestablish\textbf{mentarianism}
		\item \textbf{Infixes} -- in the middle of the stem. Hingi (borrow) to h\textbf{um}ingi (borrower) in Tagalog
		\item \textbf{Circumfixes} -- at the start and end of stem. Sagen(say) to \textbf{ge}sag\textbf{t} (said) in German
	\end{itemize}
	\item \textbf{Clitic} -- morpheme which functions \textbf{syntactically like a word}, but does not appear as an individual word
\end{itemize}

\textbf{"unladylike"} is a word with 3 morphemes and 4 syllables. The three morphemes are:
\begin{enumerate}
	\item "un-" -- means "not" (derivational affix)
	\item "lady" -- means "(well behaved) female adult human" (root)
	\item "-like" -- having the characters of (derivational affix)
\end{enumerate}
These three units cannot be broken down further \textit{without distorting the meaning of the units into something they're not}. The \textbf{stem} of "unladylike" is "unladylike" , since no inflectional affixes are used.

\textbf{"technique"} has a single morpheme and two syllables. The root and stem are therefore "technique" as well.

\textbf{"dogs"} has two morphemes and one syllable. The two morphemes are:
\begin{enumerate}
	\item "dog" -- animal (root)
	\item "-s" -- plural marker on nouns (inflectional)
\end{enumerate}
 The \textbf{stem} of "dogs" is "dog", as the inflectional affix "-s" has been removed.
 
\subsubsection{Inflectional}

\textbf{Inflection} is a variation in the form of a word that expresses a grammatical contrast. It doesn't change the word class but causes the word to serve a \textbf{new grammatical role}. Inflection adds tense, number/counts, person, mood and aspect to words.

The change is usually achieved by adding an \textbf{inflectional affix}. This affix usually produces a predictable change of meaning. That is, the change is consistent across most/all words. For example, "-s" is an inflection affix which typically pluralises nouns and makes transforms a verb:
\begin{quote}
	\textit{"apple -$>$ apples"}
\end{quote}
\begin{quote}
	\textit{"run -$>$ runs"}
\end{quote}

\subsubsection{Derivational}

\textbf{Derivation} is the formation of a new word or inflectable stem from another word or stem. An example of this is:
\begin{quote}
	\textit{"compute -$>$ computer -$>$ computerisation"}
\end{quote}
The two latter words contain the same root "compute", but through derivational affixes "-r" and "-isation" mean different things.

Derivation can \textbf{change the word class}, such as verb $\rightarrow$ noun and noun $\rightarrow$ adjective. For example, "compute" is a verb which is transformed into the noun "computer" by adding "-r".

\subsection{Stemming}

\textbf{Stemming} is the process of taking a word, complete with all its derivational and inflectional affixes and finding the \textbf{stem} of the original word. That is, a stemmer removes all inflectional affixes while leaving the stem intact.

Rule-based stemmers stem words based on rules. An example of this is the \textbf{Porter stemmmer}, which simply hacks off the end of the use. It is frequently used, especially for information retrieval, but the results are ugly.

Original:
\begin{quote}
Pierre Vinken , 61 years old , will join the board as a nonexecutive
director Nov. 29 . Mr. Vinken is chairman of Elsevier N.V. , the Dutch
publishing group . Rudolph Agnew , 55 years old and former chairman of
Consolidated Gold Fields PLC , was named a nonexecutive director of
this British industrial conglomerate . A form of asbestos once used to
make Kent cigarette filters has caused a high percentage of cancer
deaths among a group of workers exposed to it more than 30 years ago ,
researchers reported.
\end{quote}

Porter stemmed version:
\begin{quote}
Pierr Vinken , 61 year old , will join the board as a nonexecut
director Nov. 29 . Mr. Vinken is chairman of Elsevi N.V. , the Dutch
publish group . Rudolph Agnew , 55 year old and former chairman of
Consolid Gold Field PLC , wa name a nonexecut director of thi British
industri conglomer . A form of asbesto onc use to make Kent cigarett
filter ha caus a high percentag of cancer death among a group of
worker expos to it more than 30 year ago , research report .
\end{quote}

While the results are ugly, it might not matter. Bad stemming is not necessarily a problem for \textbf{automated information retrieval} as long as the stemming is \textit{consistently} bad/incorrect. If so, then the automated document searching, which is based on keywords that also get stemmed using the same stemmer, will still work efficiently, taking into consideration correct word stems (correct in that they are consistently incorrect).

WordNet's morphy() is a slightly more sophisticated rule-based stemmer. It uses different rules based on the part-of-speech tag of a word and has an exception list for irregular words. Some of the rules are listed in Figure\ref{fig:morphy-rules}

One approach is to "cheat" and not use rules at all. Instead,, all variants of a stem are stored in a \textbf{dictionary}. Stepping a word is simply a matter of looking up that word in a dictionary and retrieving the associated stem.

\textbf{Categorical Variation Database (CatVar)} is a "database of clusters of uninflected words and their categorical (i.e. part-of-speech) variants".

Rule-based systems and dicitonary systems cannot \textbf{generalise} to different languages. \textbf{Unsupervised Machine learning}, on the other hand, can be used to find recurring patterns in languages which researchers can use to understand how to stem words in any language better.

\section{Counting Words}

\subsection{Frequency Distributions}

\textbf{Word frequency} is the number of types a token type appears in the text. \textbf{Absolute frequency} is the number of times a word occurs in a body of text. \textbf{Relative frequency} is the number of times a word occurs in a body of text, \textbf{relative} to all the other words in the text (i.e. the text's size). Relative frequency is computed using Equation \ref{eq:rel-freq}, where $f(x)$ is the frequency of word $x$ and $T$ is a list of all the tokens/words in the document.

\begin{equation}
	rf(x) = \frac{frequency\;of\;x}{total\;frequency} =
	\frac{f(x)}{ \sum_{y \in T} {f(y)} }
	\label{eq:rel-freq}
\end{equation}

The frequencies of all word \textit{types} can be counted and put in a \textbf{frequency distribution}, which is table of word types and their frequencies (\textbf{ordered} from most frequency to least). Table \ref{tab:freqdist} and Figure \ref{fig:freqdist} shows an examples of frequency distribution tables and plots.

\begin{table}
	\centering
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Token} & \textbf{Frequency} \\
		\hline
		the & 350 \\
		and & 212 \\
		to & 191 \\
		of & 167 \\
		a & 165 \\
		i & 160 \\
		that & 134 \\
		... & ...\\
		but & 68 \\
		... & ...\\
		donald & 1 \\
		\hline
	\end{tabular}
	\caption{Example Frequency Distribution Table (ordered from most-frequent to least}
	\label{tab:freqdist}
\end{table}

TODO: frequency distribution example plot

\subsection{Zipf's Law}

The \textbf{rank} of a word is its position in an ordered frequency distribution table. If a word has a rank of 1, it is the \textbf{most frequent} word in the text.

\textbf{Zipf's law} captures the relation between the frequency and rank of a word. Let $r$ and $f$ be the rank and frequency of a word. There is a constant $k$ such that:
\begin{equation}
	k = f \cdot r
\end{equation}
Alternative, $f$ is proportional to $r$ like so:
\begin{equation}
	f \propto \frac{1}{r}
\end{equation}

Figure \ref{fig:log-zipf} is a logorithmic plot showing this relation. Figure \ref{fig:zipf-corpus} plots the frequency distribution of a one million word corpus (rank/freq, not word itself) against Zipf's law. As evident by these plots, Zipf's law does apply to real, natural text.

TODO: two Zipf plots

So what does this mean?
\begin{itemize}
	\item There is a \textbf{very small number of very common words}
	\item There is a small-medium number of middle frequency words
	\item There is a \textbf{very large number of words which are infrequent} (especially words with a frequency of one)
	\item The relationship between frequency and rank can be \textbf{approximated by a line} (in logarithmic scales)
	\item This is different from the bell curve/normal distribution (which is a common statistical distribution for real word data)
\end{itemize}

Zipf's law highlights a common issue that affects many NLP techniques. \textbf{Data sparseness} refers to the fact that most words will have very few, or no, examples in training data. This leads to unreliable frequency counts and probabilities, which are often used in NLP.

\section{Information Retrieval}

\subsection{Definition}

Information retrieval (IR) addresses the following task:
\begin{quote}
	given a query, find documents that are "relevant" to the query
\end{quote}

The problem has the following inputs and outputs:
\paragraph{} \begin{tabular}{ll}
	\textbf{Input:} & a large, static document collection \\
	\textbf{Input:} & keyboard-based query \\
	\textbf{Output:} & find all documents relevant to query, and \textbf{only} those relevant documents \\
\end{tabular}

A \textbf{document} here refers to a body of text, but IR extends beyond items of text -- it can be any item you want to find (e.g. image, video, etc.) A document is described by a set of \textbf{index terms}, which in this case are the words in the document. 

\paragraph{}

Example uses of IR systems include:
\begin{itemize}
	\item search set of abstracts (of research papers)
	\item search newspaper article
	\item library search
	\item search the web
\end{itemize}

\subsection{Architecture}

Figure \ref{fig:ir-architecture} shows the architecture of a typical IR system. IR systems have \textbf{three main components}:
\begin{itemize}
	\item \textbf{Query Matcher} -- matches a given query to set of documents it believes are relevant to said query
	\item \textbf{Learning Component} -- receives feedback from the user and from that the system learns about which documents are relevant to which queries/terms
	\item \textbf{Object Base} -- database of documents and their index terms (descriptions)
\end{itemize}

An example sequence of events is:
\begin{enumerate}
	\item \textbf{User} enters query "red trousers"
	\item \textbf{Query Matcher} uses that to find three documents in the \textbf{Object Base} it believes are relevant to "red trousers" and returns them to the user
	\item \textbf{User} gives feedback about which of the three documents were relevant or not (e.g. clicking on web page, explicitly clicking "not relevant" button", etc.)
	\item \textbf{Learning Component} uses this feedback to learn/alter itself to reduce the chances of the documents marked irrelevant show up the next time someone types "red trousers". This may involve updating documents' descriptions in the \textbf{Object Base}.
\end{enumerate}

TODO: architecture diagram from slide 6

\subsection{Indexing}

Automatic indexing is a process which associates documents to index terms (descriptors) for fast look-up by the query matcher. If we consider \textbf{documents} to be bodies of text and the words contained with the text to be the \textbf{index terms}, then an \textbf{inverted file structure} can be established.

First we construct a list of numbered documents and the words contained within each document, as shown in Table \ref{tab:doc-word}. This may involve removing case (e.g. all lowercase), punctuation and common words, such as "the" and "at", which have little semantic meaning (known as \textbf{stopwords}) so only meaningful tokens remain. We could even stem words or use pairs of words (bigrams, computationally expensive) for collocation information.

We then \textbf{invert} the table so it is indexed by words, instead of documents. This creates a list of words and the documents those words are present in, like the one in Table \ref{tab:word-doc}.

More sophisticated inverted file structures can be created, which can track the \textbf{frequency} of a word in a document and its \textbf{position} in the document. Table\ref{tab:word-doc-sophisticated} shows an inverted file structure which lists the \textit{index} position of the token in the document (e.g. 'cold' is the 6th word in document 1).

\begin{table}
	\centering
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Document} & \textbf{Text} \\
		\hline		
		1 & Pease porridge hot, pease porridge cold \\
		2 & Pease porridge in the pot \\
		3 & Nine days old \\
		4 & Some like it hot, some like it cold \\
		5 & Some like it in the pot \\
		6 & Nine days old \\
		\hline				
	\end{tabular}
	\caption{List of documents and the tokens contained within those documents}
	\label{tab:doc-word}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		\textbf{Number} & \textbf{Tokens} & \textbf{Documents} \\
		\hline
		1 & cold & 1,4 \\
		2 & days & 3,6 \\
		3 & hot & 1,4 \\
		4 & in & 2,5 \\
		5 & it & 4,5 \\
		6 & like & 4,5 \\
		7 & nine & 3,6 \\
		8 & old & 3,6 \\
		9 & pease & 1,2 \\
		10 & porridge & 1,2 \\		
		11 & pot & 2,5 \\
		12 & some & 4,5 \\
		13 & the & 2,5 \\		
		\hline
	\end{tabular}
	\caption{List of tokens and the documents those tokens are contained in}
	\label{tab:word-doc}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		\textbf{Number} & \textbf{Tokens} & \textbf{(Document; Word Index)} \\
		\hline		
		1 & cold & (1;6), (4;8) \\
		2 & days & (3;2), (6;2) \\
		3 & hot & (1;3), (4;4) \\
		4 & in & (2;3), (5;4) \\
		5 & it & (4;3,7), (5;3) \\
		6 & like & (4;2,6), (5;2) \\
		7 & nine & (3;1), (6;1) \\
		8 & old & (3;3), (6;3) \\
		9 & pease & (1;1,4), (2;1) \\
		10 & porridge & (1;2,5), (2;2) \\		
		11 & pot & (2;5), (5;6) \\
		12 & some & (4;1,5), (5;1) \\
		13 & the & (2;4), (5;5) \\	
		\hline
	\end{tabular}
	\caption{List of tokens and the documents those tokens are contained in (augmented with the \textbf{location} of the token in the document(s))}
	\label{tab:word-doc-sophisticated}
\end{table}

TODO: FIGURE FROM slide 3 of Eric

TODO: discussion topics (as many docs as possible??)

Indexing as many documents as possible sounds like a good way to provide a \textbf{more comprehensive search}, but it may not be the thing to do.  There are \textbf{disadvantages}:
\begin{itemize}
	\item if you only want to look at a certain type of document, it is much more difficult to get precise, relevant hits when everything but the kitchen sink is in the system
	\item more documents means greater storage requirements and slower processing/search times
	\item more documents might not always given you \textit{new} information. You could have five different documents which all contain the same information -- why have all three?
\end{itemize}

\subsection{Notation}

We now deal with the \textbf{Query Matcher} component of the IR system. That is, how do we find documents relevant to a given a \textbf{search query}?

\paragraph{}

\begin{tabular}{p{4cm}p{12cm}}
	$D$ & list of $m$ documents \\
	$m$ & number of documents \\
	$T$ & list of index terms (words) \\
	$n$ & number of index terms \\
	$w_{ij}$ & \textbf{weight of association} between the $i$th index term $i$ and the $j$th document \\
	$d_j = (w_{1,j}, w_{2,j}, ..., w_{i,j})$ & Index term vector for $j$th document, which contains the weights between all index terms and document $j$.
\end{tabular}

\paragraph{}

\begin{tabular}{p{16cm}}
	\textbf{Example} \\
	$T = \lbrace pudding, jam, traffic, lane, treacle \rbrace$ \\
	$d_1 = (1, 1, 0, 0, 0)$ \\
	$d_2 = (0, 0, 1, 1, 0)$ \\
	$d_3 = (1, 1, 1, 1, 0)$ \\
\end{tabular}

\subsection{Set Theoretic Model}

The \textbf{set theoretic} model uses set theory to find documents which uses \textbf{boolean expressions} as search queries, like the one in Equation \ref{eq:set-theoretic-query}. After receiving such a query, the following steps are performed:
\begin{enumerate}
	\item the query is transformed into \textbf{Disjunctive Normal Form (DNF)} (example in Equation \ref{eq:set-theoretic-dnf})
	\item let $R = \emptyset$ be the set containing
	\item for each document $d_j = (w_{1,j}, w_{2,j}, ..., w_{i,j})$:
	\begin{enumerate}
		\item take the truth values (weights) in $d_j$ and check if it matches \textit{any} component of the DNF equation. This process is denoted using Equation \ref{eq:set-theoretic-similarity}
		\item if the output of the DNF expression was 1, then the document matches the original boolean query exactly. Add $d_j$ to $R$
	\end{enumerate}
	\item return $R$
\end{enumerate}

\begin{equation}
	(Jam \lor Treacle) \land Pudding \land \neg{Lane} \land \neg{Traffic}
	\label{eq:set-theoretic-query}
\end{equation}

\begin{equation}
	(1, 1, 0, 0, 0) \lor (1, 0, 0, 0, 1) \lor (1, 1, 0, 0, 1)
	\label{eq:set-theoretic-dnf}
\end{equation}

\begin{equation}
	sim(d, q_{DNF}) = \begin{cases}
		1,& \text{if }d\text{ is equal to any component of }q_{DNF} \\
		0,& \text{otherwise}
	\end{cases}
	\label{eq:set-theoretic-similarity}
\end{equation}

\paragraph{\textbf{EXAMPLE}} Taking the three documents $d_1$, $d_2$ and $d_3$, as well as the query and its DNF equivalent in Equations \ref{eq:set-theoretic-query} and \ref{eq:set-theoretic-dnf}, an example of using the set theoremtic model to find which documents are relevant to the query is given. Steps:
\begin{enumerate}
	\item Does $d_1 = (1, 1, 0, 0, 0)$ match a component in the DNF equation? \textbf{Yes}, $R = R \cup \lbrace d_1 \rbrace$.
	\item Does $d_2 = (0, 0, 1, 1, 0)$ match a component in the DNF equation? \textbf{No}, don't add $d_2$ to $R$.
	\item Does $d_3 = (1, 1, 1, 1, 0)$ match a component in the DNF equation? \textbf{No}, don't add $d_3$ to $R$.
\end{enumerate}
$R =  \lbrace d_1 \rbrace$, so only one relevant document is returned ($d_1$).

TODO: in-place figure showing the venn diagram from Eric's slides (second venn)

TODO: put Katja's venn diagram here too

\subsubsection{Summary}

\paragraph{\textbf{Advantages}}
\begin{itemize}
	\item Boolean model is simple and queries have \textbf{precise semantics}, giving user complete control
	\item Popular with bibliographic systems and also available on some search engines
	\item Set-theoretic model can be extended to form a partial-matching system using \textbf{Fuzzy set model} and the \textbf{extended Boolean model}
\end{itemize}
\paragraph{\textbf{Disadvantages}}
\begin{itemize}
	\item Only returns \textbf{exact matches}. What if the user didn't know exactly what they were looking for or how to phrase it?
	\item Users find boolean queries hard to formulate
\end{itemize}

\subsection{Vector Space Model}

The \textbf{vector space model} treats documents as points in \textbf{high dimensional space}. Queries are also represented in vector space. Documents with the highest document-query similarity are selected. An \textbf{ordered list of relevant documents} is returned, going from \textbf{most relevant to least} (based on the similarity measure used).

Figure \ref{fig:vector-space-model} shows an IR system with three documents and two index terms, "car" and "insurance". The three documents and the given input query are represented as vectors in the space of index terms (which is 2D in this case as there are two terms). $d_3 = (5, 1)$ because it has 5 occurrences of insurance and 1 occurrence of car. This means that the \textbf{document-term weights are no longer binary values}.

Therefore, an $m \times n$ \textbf{document-by-word} matrix $M$ is constructed, where the rows are documents, the columns are terms and entry $M_{ij}$ is the \textbf{frequency} of the $j$th term appearing in the $i$th document. A query is a \textbf{vector} $q$ of size $n$, which contains weightings for each index term. An example of these is shown in Tables \ref{tab:doc-by-word-vector-space} and \ref{tab:query-vector-space}.

\begin{table}[H]
	\centering
	\begin{tabular}{|p{1cm}|p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}|}
	\hline
	& $Term_1$ & $Term_2$ & $Term_3$ & ... & $Term_n$ \\
	\hline
	$Doc_1$ & 14 & 6 & 1 & ... & 0 \\
	$Doc_2$ & 0 & 1 & 3 & ... & 1 \\
	$Doc_3$ & 0 & 1 & 0 & ... & 2 \\
	... & ... & ... & ... & ... & ... \\
	$Doc_n$ & 4 & 7 & 0 & ... & 5 \\
	\hline
	\end{tabular}
	\caption{Example Document-By-Word Matrix with Word Frequencies}
	\label{tab:doc-by-word-vector-space}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{|p{1cm}|p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}|}
	\hline
	$q$ & 0 & 1 & 0 & ... & 1 \\
	\hline
	\end{tabular}
	\caption{Example Query Vector with Weightings for each Index Term}
	\label{tab:query-vector-space}
\end{table}

\subsubsection{Similarity Measure}

A \textbf{similarity measure} $sim(\vec{d}, \vec{q})$, which compares the similarity between a document $\vec{d}$ and a search query $\vec{q}$ must be given. The similarity measure uses throughout this module summary is in Equation \ref{eq:cosine-similarity}. This is the \textbf{cosine} of the two vectors and measures the angle between two vectors in vector space. \textbf{Smaller angles} between the documents mean they are more similar and the opposite for larger angles. This also \textbf{normalises} the output so long documents don't \textbf{skew the ranked list} of relevant documents with larger values.

\begin{equation}
	sim(\vec{d}, \vec{q}) = cos(\vec{d}, \vec{q}) = \frac{\vec{d} \cdot \vec{q}}{|\vec{d}| |\vec{q}|}
	\label{eq:cosine-similarity}
\end{equation}

Figure \ref{fig:cosine-similarity-example} illustrates how this similarity measure \textbf{relates to} the spatial representation of the documents and query. Table \ref{tab:cosine-similarity-example} and Equation \ref{eq:cosine-similarity-example} show the cosine similarity function applied to two documents, given a query $q$.

\begin{table}[H]
	\centering
	\begin{tabular}{|r|l|l|l|}
		\hline
		& $q$ & $d_{7655}$ & $d_{454}$ \\
		\hline
		hunter & 19.2 & 56.4 & 112.2 \\
		gatherer & 34.5 & 122.4 & 0 \\
		Scandinavia & 13.9 & 0 & 30.9 \\
		30,000 & 0 & 457.2 & 0 \\
		years & 0 & 12.4 & 0 \\
		BC & 0 & 200.2 & 0 \\
		prehistoric & 0 & 45.3 & 0 \\
		deer & 0 & 0 & 23.6 \\
		rifle & 0 & 0 & 452.2 \\
		Mesolithic & 0 & 344.2 & 0 \\
		\hline
	\end{tabular}
	\caption{Two documents and a query vector represented in 10D term space, with weights for each 10 terms}
	\label{tab:cosine-similarity-example}
\end{table}

\begin{multline}\\
	sim(d_{7655}, q) = cos(d_{7655}, q) = \frac{d_{7655} \cdot q}{|d_{7655}| |q|} = \frac{5305.68}{41.86(622.86)} = \frac{5305.68}{26071.72} = 0.20 \\
	sim(d_{4655}, q) = cos(d_{454}, q) = \frac{d_{454} \cdot q}{|d_{454}| |q|} = \frac{2583.75}{41.86(467.53)} = \frac{2583.75}{19569.97} = 0.13 \\
	cos^{-1}(0.20) = 78.46^{\circ}
	\;\;\;\;\;\;\;\;\;\;\;
	cos^{-1}(0.13) = 82.53^{\circ} \\
	\label{eq:cosine-similarity-example}
\end{multline}

\subsubsection{Term Weighting}

So how do we decide what \textbf{weights} to given each index term in the documents and in the search query $q$? We could use the \textbf{frequency} of the term in documents, but what if the term is generally frequent in \textbf{all documents}? Then, it doesn't really you find relevant documents, as all documents contain the word. This highlights a key issue:
\begin{quote}
	\textbf{Not all terms describe a document equally well!}
\end{quote}

\paragraph{}

\begin{tabular}{ll}
	\textbf{Definitions} & \\
	$tf_{w,d}$ & number of times word $w$ occurs in document $d$ \\
	$df_w$ & number of documents that contain $w$ \\
	$|D|$ & total number of documents in IR system 
\end{tabular}

Terms which are \textbf{frequent in a document} are better: $tf_{w,d} = freq_{w,d}$. Terms that are \textbf{rare in the overall document} collection are better: $idf_{w,D} = \log_{10} \frac{|D|}{df_w}$. Therefore, we combine these two measurements using Equation \ref{eq:tf-idf} to formulate a measurement of how \textbf{useful a term is at describing  a specific document}. This measurement is called \textbf{TF/IDF}.

\begin{equation}
	tfidf_{w,d,D} = tf_{w,d} \cdot idf_{w,D} = freq_{w,d} \cdot \log_{10} \frac{|D|}{df_w}
	\label{eq:tf-idf}
\end{equation}

This combined measurement gives high weighting to frequent terms, but \textbf{penalises} terms which are frequent in \textit{all} documents, as they are less helpful in distinguishing which documents are relevant and which are not).

Table \ref{tab:tf-idf} lists some terms and their frequency within a document $d$ and the whole document collection $D$. It also contains the TF/IDF measurement for each word. Notice how "general" has low frequency compared to "the", but since it appears in much less documents in the overall collection, it has a much higher weighting when using TF/IDF. The process of computing the TF/IDF value for the word "the" is shown in Equation \ref{eq:tf-idf-example}.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		\textbf{Term} & $tf_{w,d}$ & $df_w$ & $|D|$ & TF/IDF ($tfidf_{w,d,D}$) \\
		\hline
		the & 312 & 28,799 & 30,000 & 500 \\
		in & 179 & 26,452 & 30,000 & 9.78 \\
		general & 136 & 179 & 30,000 & 302.50 \\
		fact & 131 & 231 & 30,000 & 276.87 \\
		explosives & 63 & 98 & 30,000 & 156.561 \\
		nations & 45 & 142 & 30,000 & 104.62 \\
		haven & 37 & 227 & 30,000 & 78.48 \\
		\hline
	\end{tabular}
	\caption{Table showing frequencies of words for a specific document $d$ for a collection of documents $D$. The TF/IDF index of each word for document $D$ is given.}
	\label{tab:tf-idf}
\end{table}

\begin{multline}\\
	idf_{the,D} = \log_{10} \frac{30,000}{28,799} = 0.0178 \\
	tf_{the,d} = freq_{w,d} = 312 \\
	tfidf_{the,d,D} = tf_{the,d} \cdot idf_{the,D} = 312 \cdot 0.0178 = 5.55 \\
	\label{eq:tf-idf-example}
\end{multline}

\subsubsection{Summary}

\paragraph{\textbf{Advantages}}
\begin{itemize}
	\item The vector space model is simply, \textbf{fast} and leads to "good" results
	\item \textbf{Partial matching} makes it more likely the user will get a hit, and also allows for ranked list of outputted documents
	\item Popular model for search engines
\end{itemize}
\paragraph{\textbf{Disadvantages}}
\begin{itemize}
	\item Makes the assumption that terms are \textbf{independant}. This is \textbf{not realistic}, as natural language has phrases, collocations and grammar.
\end{itemize}

\subsection{Precision Cutoff}

\textbf{Precision} and \textbf{recall} are ways of evaluating the effectiveness of IR systems, as described in Section \ref{sec:ir-evaluation}. Recall and precision change depending on the number of relevant documents returned by the IR system.

Take the example in Figure \ref{fig:precision-cutoff}. The precision for the first ranking after running through the \textbf{first five documents in order} is 1.0. The second ranking has a precision of 0, since the first five documents are all non-relevant (but there are five relevant documents after that!) When counting relevant/non-relevant documents through the \textit{entire} ordered document list, the precision will be same regardless of order.

\textbf{Precision cut-off} is when you only return a pre-defined \textbf{maximum} of documents. This may improve precision, but recall is reduced (as less documents are returned). Therefore, it is important to \textbf{tweak} the point at which you stop counting to optimise recall and precision to where you want them. Too short a cut-off and precision may appear \textit{low}, as it hasn't had a chance to sample enough returned documents. Too long and there will be many irrelevant documents, which will also result in low precision.

TODO: precision cutoff figure

\textbf{Average precision} is another way of measuring precision, which \textbf{averages} the precision measurements from using different cut-off values.An example of this is shown in Figure \ref{fig:average-precision-figure}.

TODO: average precision figure

\subsection{Manipulation of IR Systems}

One big limitation with IR systems is that they're often \textbf{easy to manipulate}. Many people exploit web search engines to get their pages high in searches, for example. One might hide loads of terms in their web pages which fool IR systems in thinking the page is relevant when it's not (causing the user to visit it). This is why most systems' query ranks are \textbf{supplemented by popularity measures} (e.g. page hits), so that kind of abuse is prevented.

\subsection{IR vs. Databases}

So what stops us from simply using a \textbf{DBMS (Database Management System)} to store documents and just using database queries to find what we want? Depending on the system and use case, it may actually make sense to do that instead. Table \ref{tab:ir-db-comparison} compares the two different approaches. The core difference is that DBMSes require exact queries to be inputted and output exact matches. IR systems are less sensitive to variance, requiring a loose query from the user and outputs \textit{likely} matches.
 
\begin{table}
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		& \textbf{DBMS} & \textbf{IR} \\
		\hline
		\textbf{match} & exact & partial or best match \\
		\textbf{inference} & deduction & induction \\
		\textbf{model} & record/field & text document \\
		\textbf{query language} & artificial & natural? \\
		\textbf{query specification} & complete & incomplete \\
		\textbf{items wanted} & matching & relevant \\
		\textbf{error response} & sensitive & insensitive \\
		\hline
	\end{tabular}
	\caption{Comparison of Information Retrieval to DBMS for Document Searching}
	\label{tab:ir-db-comparison}
\end{table}

\subsection{Query Broadening}

TODO: main idea???

\subsubsection{Relevance Feedback}

TODO: formulae

TODO: example

TODO: PROS/CONS

todo: positive and negative feedback

\subsubsection{Thesaurus/Ontology}

TODO: definition

TODO: synonyms, hypernyms, hyponyms

TODO: coordinate terms

\subsubsection{Language Normalisation}

TODO: what this is with architecture

TODO: example

TODO: how it uses ontology (thesaurus use)

\section{Text Classification}

Text classification can be thought as a \textbf{standing query}, which is when you run a query periodically to find \textbf{new} items. The query does not rank items but \textbf{classifies} them as relevant vs. not relevant (to a specific topic). This section will focus on \textbf{topic classification}. An example of topic classification isdetermining if an email is spam or not, or what topic a news article is about (e.g. technology, economics, biology, etc.).

\paragraph{}

\begin{tabular}{ll}
\textbf{Problem Definition} \\
\textbf{Given} & a representation of a \textbf{document} $d$ \\
& a fixed set of \textbf{classes} (or labels or categories) $C = \lbrace c_1, c_2, ..., c_j \rbrace$ \\
\textbf{Determine} & \textbf{category} of $d$, $\gamma(d) \in C$ \\
& where $\gamma$ is a \textbf{classification function} that maps documents onto classes \\
\end{tabular}

\subsection{Classification Methods}

Documents can be \textbf{manually} by humans. This typically results in higher accuracy, but is much, much slower. \textbf{Hand-coded rule-based} classifiers are also possible, but to correctly classify even the simplest of documents, very precise rules are required. The rules must constantly evolve to handle outliers/corner cases that are always found. It's very hard to do in the long term.

Another approach is to use \textbf{supervised machine learning}, which produces a classification function based on \textbf{training dataset}. This is described more in Section \ref{sec:bow-text-classification}.

\subsection{BOW Text Classification}
\label{sec:bow-text-classification}

\begin{tabular}{p{7cm}p{10cm}}
\textbf{Supervised Machine Learning} \\
\textbf{Definition} \\
\textbf{Given} & a (test) document $d$ \\
	& a fixed set of \textbf{classes} $C = \lbrace c_1, c_2, ..., c_j \rbrace$ \\
	& a training set $D$ of documents, each with a label in $C$. That is, $D = \lbrace (d_1, c_1), (d2, c_2), ..., (d_m,c c_m) \rbrace$ \\
\textbf{Determine} & a \textbf{learning method} which will enable use to learn a classifier $\gamma$ \\
	& assign $d$ a class in $C$, or $\gamma(d) \in C$
\end{tabular}

The \textbf{bag of words (BOW)} representation of a document which discards all positioning, ordering and structure from a text document. It is simply a list of words which appear in the document and the number of times (frequency) they occur.

For topic classification, a document $d$ will use a BOW representation by being a word frequency table, with the frequency of each word type in the overall \textbf{vocabulary}.

\subsection{Naive Bayes}

\textbf{Naive Bayes} is a type of \textbf{probabilistic} classifier that uses Bayes' theorem (Equation \ref{eq:bayes-theorem}. This classifier used widely for topic classification.

\begin{equation}
	P(A|B) = \frac{P(B|A) P(A)}{P(B)}
	\label{eq:bayes-theorem}
\end{equation}

For a document $d$ that uses a BOW representation, we assign it \textbf{most likely class} $c \in C$. We can compute the probability of $d$ being a class $c_i$ using \textbf{Bayes' theorem/rule}. Two types of classifier will be considered -- multinomial and binomial

\subsubsection{Multinomial Naive Bayes}

\textbf{Multinomial Naive Bayes} takes the frequencies of words in the document into consideration (but not position. Equation \ref{eq:multinomial-naive-bayes-base} shows how this, combined with Bayes' rule and BOW, can be used to find the most likely class for a document $d$.
 
\begin{equation}
\begin{aligned}[b]
	C_{d} &= max_{c \in C} \; P(c|D) \\
	&= max_{c \in C} \; \frac{P(d|c)P(c)}{P(d)} 
	\;\;\;\;\;\;\;\;\;\;\;\;\; \text{by Bayes' rule} \\
	&= max_{c \in C} \; P(d|c)P(c) 
	\;\;\;\;\;\;\;\;\;\;\;\;\; \text{we \textit{know} we have document }d\text{, so } P(d) = 1 \\
	&= max_{c \in C} \; P(w_1,w_2,...,w_{n_d}| c)P(c)
	\;\;\;\;\;\;\;\;\;\;\;\;\; \text{expanding }P(d|c) \\	
	&= max_{c \in C} \; P(X_1 = w_1, X_2 = w_2, ..., x_{n_d} = w_{n_d} | c)P(c)
	\;\;\;\;\;\;\;\;\;\;\;\;\; \text{adding position probabilities} \\
\end{aligned}
\label{eq:multinomial-naive-bayes-base}
\end{equation}

So the probability document $d$ is in class $c$ is given by $P(X_1 = w_1, X_2 = w_2, ..., x_{n_d} = w_{n_d} | c)P(c)$. Naive Bayes makes two assumptions that makes computing this formula easier -- conditional independence and a Bag of Words assumption.

\textbf{Conditional independence} assumes that the words are conditionally \textbf{independent} of the given class $c$.  This means the likelihood of a sequence of words (document) being a document of class $c$ can simply be the \textbf{product} of all of the individual word probabilities, as shown in Equation \ref{eq:conditional-independence}.

\paragraph{\textbf{NOTE}}: $P(X_i = w_j)$ means the probability of word$w_j$ being at position $i$ in the document, so $P(X_i = w_j|c)$ means the probability of word $w_j$ being at position $i$ \textbf{given} that the class is $c$.

\begin{equation}
	P(X_1 = w_1, X_2 = w_2, ..., x_{n_d} = w_{n_d} | c) = P(X_1 = w_1|c) \cdot P(X_2 = w_2 | c) \cdot ... \cdot P(x_{n_d} = w_{n_d}|c)
	\label{eq:conditional-independence}
\end{equation}

The \textbf{Bag of Words} assumption assumes the position of words \textbf{does not matter}. Equation \ref{eq:bow-assumption} shows that this means the probabilities of a word occurring at \textbf{any position is the same}.

\begin{equation}
	P(X_j = w | c) = P(X_l = w| c)  \;\;\;\;\;
	\forall \; c,j,l,w
	\label{eq:bow-assumption}
\end{equation}

This means we only need to compute \textbf{two kinds of probabilities}:
\begin{itemize}
	\item probability of class $P(c)$
	\item probability of word $i$ occurring, \textit{given} the document is class $c$ $P(w_i|c)$
\end{itemize}
$P(c)$ is computed using Equation \ref{eq:naive-bayes-class-prob}. Concatenate all training documents of class $c$ into one \textbf{large document}, call it $L_c$. $P(w_i|c)$ is computed using Equation \ref{eq:naive-bayes-word-prob}, where $n_i^c$ is the frequency of word $w_i$ in $L_c$ and $n^c$ is the \textbf{length of the large document} $|L_c|$.

\begin{equation}
	P(c) = \frac{numDocsOfClassC}{totalNumDocsInTraining}
	\label{eq:naive-bayes-class-prob}
\end{equation}

\begin{equation}
	P(w_i|c) = \frac{n_i^c}{n^c}
	\label{eq:naive-bayes-word-prob}
\end{equation}

Since the probability of a document being a given class is computed by multiplying individual word probabilities together, if a \textit{single} world probability is 0, then the probability of the document being that class is 0. This is not what we want; what if the word is just unseen and the rest of the document are words that are representative of the class in question? This skews the results too much.

\textbf{Smoothing} is done on the individual word probabilities to ensure there are never 0 probabilities. Equation \ref{eq:naive-bayes-word-prob-smoothed} shows one way of smoothing, where $V$ is a vocabulary size (number of token types in training dataset). Now if a word has not occurred in any document of the class in question, then it will has a very low probability, but won't have 0 to corrupt the final document probability.

\begin{equation}
	P(w_i|c) = \frac{n_i^c + 1}{n^c + |V|}
	\label{eq:naive-bayes-word-prob-smoothed}
\end{equation}

Finally, using \textbf{multinomial Naive Bayes}, the class of a document $d$ is computed using Equation \ref{eq:multinomial-bayes}, where $P(c)$ and $P(w_i|c)$ are computed as described in this section.
\begin{equation}
	C_{d} = \left( max_{c \in C} \;\; P(c) \cdot \prod_{i \in positions} P(w_i|c) \right)
	\label{eq:multinomial-bayes}
\end{equation}

To summarise, multinomial Naive Bayes:
\begin{itemize}
	\item uses conditional independance assumptions
	\item uses BoW model, ignoring position of words when estimating probabilities 
	\item sees all training documents of one class as \textbf{one long document}
	\item cares about the \textbf{frequencies} of word occurrences.
	\item \textbf{ignores} words in test document that \textbf{do not occur}. There is no \textit{negative} evidence to reduce probabilities, only \textit{positive} evidence to increase it
\end{itemize}

TODO: example from coursework

\subsubsection{Binomial Naive Bayes}

\textbf{Binomial Naive Bayes} does not take the frequency of words in the document into consideration, it only cares if the word occurs in the document at all. Equation \ref{eq:binomial-naive-bayes-base} shows how this, combined with Bayes' rule and BOW, can be used to find the most likely class for a document $d$.
 
\begin{equation}
\begin{aligned}[b]
	C_{d} &= max_{c \in C} \; P(c|D) \\
	&= max_{c \in C} \; \frac{P(d|c)P(c)}{P(d)} 
	\;\;\;\;\;\;\;\;\;\;\;\;\; \text{by Bayes' rule} \\
	&= max_{c \in C} \; P(d|c)P(c) 
	\;\;\;\;\;\;\;\;\;\;\;\;\; \text{we \textit{know} we have document }d\text{, so } P(d) = 1 \\
	&= max_{c \in C} \; P(e_1,e_2,...,e_{|V|}| c) P(c)
	\;\;\;\;\;\;\;\;\;\;\;\;\; \text{expanding }P(d|c) \\	
	&= max_{c \in C} \; P(c) \prod_{w_i \in V} P(e_i|c}
	\;\;\;\;\;\;\;\;\;\;\;\;\; \text{rearranging} \\
\end{aligned}
\label{eq:binomial-naive-bayes-base}
\end{equation}

TODO: summary

\subsubsection{Multinomial vs. Binomial}

TODO

\subsubsection{Advantages/Disadvantages}

TODO: of Naive Bayes

\subsection{Feature Selection}

TODO: why feature selection

\subsubsection{Frequency Cutoff}

TODO

\subsubsection{Mutual Information Cutoff}

TODO: definition of MU mathematically and what it means

TODO: example!

TODO: its effect on performance

\subsection{Multi-class Text Classification}

TODO: > 2 classes

\subsection{Evaluating Multi-class Classifiers}

TODO: micro and macro averaging

\subsection{Examples of Binary/Multiclass Classification}

TODO: EXAMPLES of text classification from end of first+second sets of slides

\subsection{Semantic Similarity}

TODO: automatically determine which words mean similar things is goal!

\subsubsection{Vector Space Model}

TODO: what is it and how to use it to derive similar words

\subsubsection{Context Features}

TODO: using context (n-grams???)

TODO: size of context

\subsubsection{Use of Association Measure}

TODO

\subsubsection{Similarity Measures}

TODO: similarity measures (cosine, Euclidean)

TODO: actual example

TODO: evaluation of similarity measures

\subsubsection{Clustering}

TODO: clustering approaches

TODO: evaluation of clustering approaches

\section{Evaluation Measures}

\subsection{Information Retrieval}
\label{sec:ir-evaluation}

TODO: recall, precision, f-measure

\subsection{Classification}

(slightly different perspective to IR)
TODO: recall, precision, f-measure


\section{$n$-Gram Modelling}

TODO: what it's used for

\subsection{Probabilistic Language Modelling}

TODO: assign probability of sentence

TODO: context and Markov assumption

\subsection{$n$-Gram Models}

TODO: definition of n-grams with diagram

TODO: advantages and disadvantages

TODO: why it works!!! (Penn Treebank)

\subsection{Deriving Language Model}

TODO: deriving probabilistic model of $n$-grams from frequency counts

TODO:

\subsection{Limitations of MLE}

TODO: limitations of MLE (MLE???)

TODO: how zero probabilities break down an entire model! DATA SPARSENESS

\subsection{Smoothing}

TODO: Laplace

TODO: Lidstone

TODO: Good-Turing smoothing

TODO: Class-based smoothing

TODO: web-based smoothing

TODO: summary table of all smoothings + the ones from "other" that have no detail

\subsection{Applications}

TODO: list of applications

\subsubsection{Spell Checking}

TODO: confusion sets
???

\subsubsection{Adjective Ordering}

TODO

???

\section{Information Theory}

TODO: uses

\subsection{Information and Entropy}

TODO: definition of information

TODO: definition of Entropy
TODO: 2 examples of entropy

\subsection{Joint and Condtional Entropy} 

TODO: definitions

TODO: examples

\subsection{Chain Rule}

TODO: definition

TODO: example

\subsection{Entroy Rate of Language}

TODO: language definitions

TODO: cross-entropy

TODO: entropy of English

\subsection{Mutual Information}

TODO: have the other mutual information section refer to this??

TODO: pointwise mutual information

\section{Part-of-Speech Tagging}

TODO: what is is roughlyand applications

\subsection{Word Classes}

TODO: CLASS TYPES

TODO: example class nouns + hierarchy

TODO: table showing all COMMON WORD CLASSES THAT MAY HAVE TO BE USED IN THE EXAME!!!!!!!!! SEE LAST REVISION LECTURE NOTES FOR SOME EXAMPLES OF THESE AS WELL AS SLIDES/WORKSHEETS

\subsection{Tagsets and Dictionaries}

TODO: tagset

TODO: dictionary

TODO: example sentence taggings

TODO: ambiguity

\subsection{POS Tagging as Classification}

TODO: definition of classification problem

\subsubsection{Information Sources for Tagging}

TODO: two information sources in tagging

\subsection{Markov Model Tagging}

TODO: definition of Markov model and how it's derived

TODO: example picture (state probabilities)

TODO: tag-tag model

TODO: word-tag model

TODO: examples of two models

\subsubsection{Variations}

TODO

\subsubsection{Advantages/Disadvantages}

TODO

\subsection{Transformation-Based Tagging}

TODO: general idea

TODO: templates

TODO: learning algorithm

TODO: examples

\subsubsection{Advantages/Disadvantages}

TODO

\subsection{Evaluation}

TODO: how to evaluate, current accuracies

\subsection{Caveats}

TODO: caveats of tagging


TODO: something on required reading for transformed-based tagging????????????????????????????????

\section{Grammar and Parsing}

\subsection{Constituents}

TODO: what they are and properties

\subsection{Limitations with $n$-grams and Regular Expressions}

TODO: comparison with $n$-grams - why the latter are crap

TODO: problems with regexes to parse sentences using POS tags

TODO: attachment means nested within each other, means hierarchy which regexes cannot express

\subsection{Grammar}

TODO: definitions

TODO: example grammar and using it to derive sentences (step-by-step and tree shown)

TODO: labelled bracketing and examples

TODO: mention of recursion

\subsubsection{Context-free Grammar vs. Regular Grammar}

TODO

\subsubsection{Limitations}

TODO: agreement

TODO: semantics

TODO: language dependency

\subsubsection{Context-Free Languages Enough?}

TODO

TODO: summary of grammar too

\subsection{Language Universals}

TODO

???

\subsection{Parsing}

TODO: definition of it

TODO: top-down parsing

TODO: bottom-up parsing

TODO: example of top-down

TODO: properties such parsing approaches (essentially DFS)

\subsubsection{Limitations}

TODO: can combine with bottom-up filtering to prevent rules with illegal POS taggings

TODO: infinite recursion!

TODO: effort duplication w/ example
TODO: chart parsing and how it can be used to store already seen parts of tree in table (saves effort)

\subsubsection{Summary}

TODO: summary slide augmented w/ extra details

\subsection{Global Ambiguity}

TODO: example of ambiguity

TODO: mention 3 types of ambiguity

TODO: syntactic ambiguity, catalan numbers and exponential explosion

TODO: global ambiguity, issues with this and how it can be resolved in TWO WAYS

\subsubsection{Probabilistic Context Free Grammars}

TODO: probabilistic conext free grammars + example

TODO: example that shows problem

\subsubsection{Frame Preferences}

TODO: what this is

TODO: frame probabilities and how to compute them

TODO: using frame preferences to compute likelihood of parse tree

\subsection{Local Ambiguity}

TODO: what it is

TODO: serial and parallel parsers

\subsubsection{Garden Path}

TODO: Garden Paths definition w/ ALL EXAMPLES W/ EXPLANATION!
	(each different type of ambiguity)

TODO: WHICH PARSER IS A HUMAN AND WHAT SHOULD WE USE??? THE "SO?" slide

\subsubsection{Pruning Unlikely Trees}

TODO: briefly mention Jurafsky's model

TODO: show garden path example

TODO: define beam width, state how it can used to prune trees by assuming they're garden paths IF PROBABILITY RATIO IS HIGHER THAN BEAM WIDTH!

\end{document}
