%
% Name: Natural Language Processing Coursework 1
% Author: Donald Whyte (sc10dw@leeds.ac.uk)
%

\documentclass{article}

% Make subsections use alphabet indices and not numeric indices
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\usepackage[margin=3cm]{geometry} % easy page formatting
	
\usepackage{datetime} % up-to-date, automatically generated times
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}

\title{Natural Language Processing \\ COMP3310 \\ Coursework One}
\author{Donald Whyte (sc10dw@leeds.ac.uk)}
\date{\today}

\begin{document}
\lstset{language=Python}
\lstset{basicstyle=\ttfamily}

\maketitle

\textbf{NOTE:} The code used throughout this coursework is based on the NLTK book. The training/test datasets are still a 75\%-25\% and there is no randomisation, just like the later code snippets provided however, so it shouldn't impact the results.

\section{Most Informative Features for Movie Review Classification}

In order to determine whether or not a movie review is positive or negative, a Naive Bayes classifier can be used. Using NLTK, I trained a \textbf{binomial} Naive Bayes classifier like so:

\begin{lstlisting}
# List of words to use as features for binomial classification
# This will be the 2000 most frequent tokens in the movie_reviews corpus
WORDS_TO_CONSIDER = [ ... ]

def docFeatureExtractor(text):
	text = set(text) # convert to set for faster look-up
	featureSet = {}
	for i in range(len(WORDS_TO_CONSIDER)):
		key = self.featureNames[i]
		featureSet[key] = (WORDS_TO_CONSIDER[i] in text)
	return featureSet

# Build labelled training dataset from movie_reviews corpus
trainingSet = [ ( list(movie_reviews.words(fileid)), category)
			  for category in movie_reviews.categories()
			  for fileid in movie_reviews.fileids(category) ] 
# Use feature extractor on every review
featureSet = apply_features(docFeatureExtractor, trainingSet)
# Train classifier 
classifier = nltk.NaiveBayesClassifier.train(featureSet)
\end{lstlisting}

From this, it is possible to derive which features were the most useful in distinguishing positive or negative movie reviews using the following Python command:

\begin{lstlisting}
	classifier.show_most_informative_features(30)
\end{lstlisting}
Table \ref{tab:informative_features_doc_classification} shows the 30 most informative features for classifying movie reviews as positive or negative, outputted from the above command.

\begin{table}
\centering
\begin{tabular}{|l|l|l|}
	\hline
	\textbf{Rank} & \textbf{Feature} & \textbf{Ratio} \\
	\hline
	1 & contains(outstanding) & pos : neg = 10.3 : 1.0 \\
	2 & contains(mulan) & pos : neg = 10.0 : 1.0 \\
	3 & contains(damon) & pos : neg = 7.8 : 1.0 \\
	4 & contains(seagal) & neg : pos = 6.8 : 1.0 \\
	5 & contains(wonderfully) & pos : neg = 6.6 : 1.0 \\
	6 & contains(lame) & neg : pos = 6.5 : 1.0 \\
	7 & contains(awful) & neg : pos = 6.1 : 1.0 \\
	8 & contains(patch) & neg : pos = 5.5 : 1.0 \\
	9 & contains(waste) & neg : pos = 5.2 : 1.0 \\
	10 & contains(mess) & neg : pos = 5.1 : 1.0 \\
	11 & contains(wasted) & neg : pos = 5.0 : 1.0 \\
	12 & contains(terrible) & neg : pos = 5.0 : 1.0 \\
	13 & contains(poorly) & neg : pos = 4.9 : 1.0 \\
	14 & contains(flynt) & pos : neg = 4.7 : 1.0 \\
	15 & contains(jedi) & pos : neg = 4.7 : 1.0 \\
	16 & contains(stupid) & neg : pos = 4.4 : 1.0 \\
	17 & contains(pointless) & neg : pos = 4.4 : 1.0 \\
	18 & contains(ridiculous) & neg : pos = 4.3 : 1.0 \\
	19 & contains(fantastic) & pos : neg = 4.3 : 1.0 \\
	20 & contains(worst) & neg : pos = 4.3 : 1.0 \\
	21 & contains(unfunny) & neg : pos = 4.3 : 1.0 \\
	22 & contains(allows) & pos : neg = 4.3 : 1.0 \\
	23 & contains(era) & pos : neg = 4.3 : 1.0 \\
	24 & contains(portrayal) & pos : neg = 4.1 : 1.0 \\
	25 & contains(dull) & neg : pos = 4.0 : 1.0 \\
	26 & contains(bland) & neg : pos = 3.9 : 1.0 \\
	27 & contains(laughable) & neg : pos = 3.8 : 1.0 \\
	28 & contains(terrific) & pos : neg = 3.8 : 1.0 \\
	29 & contains(julie) & neg : pos = 3.7 : 1.0 \\
	30 & contains(zero) & neg : pos = 3.6 : 1.0 \\
	\hline
\end{tabular}
\caption{30 Most Informative Features for Classifying Movie Reviews}
\label{tab:informative_features_doc_classification}
\end{table}

The majority of the features listed in table \ref{tab:informative_features_doc_classification} are adjectives that generally relate to positive or negative sentiments, such as "worst", "outstanding", "terrible" and "fantastic". It is unsurprising that these words are informative when determining whether or not a movie review is a positive or negative.

Other words are not immediately obvious, such as "zero", "waste" or "portrayal". These are not adjectives and are not \textit{directly} negative. There words tend to be used often in a specific way in movie reviews; that is, they have a specific meaning \textit{in the context of a movie review}. Therefore, it's reasonable to say that,intuitively, these would be useful identifying a review's sentiment.

There are some surprising features, however. Specific actors or characters, such as "julie", "seagal" or "flynt", and even \textit{concepts}, such as "jedi", from movies relate to the document's sentiment. One reason these tend to be useful could be that the opinion of the majority regarding those particular movies or characters is so one-sided, that during training the classifier begins to associate those terms with positive and negative sentiment.

Perhaps through such widely adopted opinion and usage of these names and terms, the semantics of these words have truly become associated to positive or negative sentiment in the English language. On the other hand, it could be an indication that the classifier has \textit{overfit} the training dataset. There may simply be a large amount of positive Star Wars reviews in the training dataset, meaning the word 'jedi' appears to be a positive word. There could also be many negative Star Wars reviews, but since the training dataset is not large or general enough to contain these, the classifier does not know. 

\section {Classification Mistakes}

\subsection{Movie Review 1 (Overfitting Training Data)}

\begin{quote}
"So this is the first time that Steven Seagal has starred in a Sci-Fi movie. I was blown away by "Sheer Space Seagal" and the amazing performances of every actor involved. I highly recommend you watch this movie."
\end{quote}

As table \ref{tab:informative_features_doc_classification} shows, the word 'seagal' tends to appear in negative movie reviews, based on the training data. Assumming 'seagal' is used in the context of Steven Seagal, a known actor in action movies, this appears to mean that the classifier has associated Seagal with bad movies.

Since the negative:positive ratio is quite high for the word 'seagal', when 'seagal' does appear in a review it strongly pushes the review towards being classified as negative. The movie review given is a very positive review of a movie that Seagal starred in. Despite the overall language being positive, the review has been classified as negative due to the occurrence of 'seagal'. A very high amount of positive evidence is required to balance out the heavy negative weighting the word 'segal' has. Since the review was classified as negative, it didn't have enough of this positive evidence.

This implies that the classifier may have \textit{overfit} the training data, identifying distinguishing features specific to the training dataset itself and not movie reviews as whole. This is the reason the classifier has not generalised well and incorrectly classifies this particular review as negative.

\subsection{Movie Review 2 (Discourse, Sentence Subject and Context)}

\begin{quote}
"The original Star Wars trilogy is a masterpiece. It took the world by storm and captivated both children and adults. It was memorable, being talked about for years, and the conclusion to Epsiode VI was outstanding.

It was the end of an era when Episode VI finished. When it was announced that George Lucas would be releasing a prequel trilogy, everyone was excited. What new adventures with the Jedi would happen in the exciting Star Wars universe?

Naturally, there's been a lot of discussion about Episode I. Everyone was queueing up and was buzzing with excitement as they were about to watch the movie.

Like many people however, when I left the movie I felt disappointed. It was nowhere near the same fantastic quality the original trilogy had. In fact, I think it was a poor movie. Remember that when going into this movie."
\end{quote}

The reason the classifier struggles to identify the following text as a negative review of Star Wars Episode I is because it does not take \textit{discourse} into account. Discourse is the relationship between sentences and text at different positions of a document.

Since the bag of words model does not take position into account, the classifier has no way of distinguishing between words about the original Star Wars trilogy from words that are about Episode I specifically. There is no mechanism for which determine \textbf{sentence subject} or \textbf{context} using the bag of words representation.

Therefore, when the writer is praising the original trilogy, the classifier just assumes that the writer is praising the movie being reviewed. This results in the classifier thinking it's a positive review, when in actual fact it's a negative review (as shown by the last paragraph).

\section{Impact of Feature Selection Methods}

There are many different ways of selecting which features or words to consider for classifiers. This section will discuss different ways of selecting features, particular used in natural language processing, and discuss their impact on classification accuracy.

\subsection{Frequency Cutoff}

The code used for the previous two questions has been using a technique called \textit{frequency cutoff} to select which features to use for the classifier. That is, which words to look for in each document.

The frequency of all the words in the training corpus is calculated, and the $k$ most frequent words are chosen as features. Table \ref{tab:frequency_cutoff} shows the accuracy of classifiers which use different amounts of features (different values for $k$). Notice how as $k$ is increased, both accuracy on the training and test datasets increase.

However, once $k$ reaches a certain point (between 3000 and 5000),  accuracies on the training dataset are still increased, but when the classifier is run on the test dataset (unseen data), the accuracy steadily decreases. This is a sign that as you increase the number of words to use for binomial Naive Bayes classification, classifiers start to overfit the training data and don't generalise well to new, unseen documents.

Therefore, care is needed when picking a value for $k$ so that the classifier achieves high enough accuracies \textit{without} overfitting the training dataset.

\begin{table}
	\centering
	\begin{tabular}{|l|l|l|}
	\hline
	\textbf{Number of Features $k$} & \textbf{Training Acc.} & \textbf{Test Acc.} \\
	\hline
	25 & 67.33\% & 2.80\% \\
	50 & 65.80\% & 22.20\% \\
	100 & 70.53\% & 33.20\% \\
	250 & 73.33\% & 57.20\% \\
	500 & 80.60\% & 67.00\% \\
	1000 & 86.47\% & 73.33\% \\
	2000 & 90.73\% & 65.20\% \\
	3000 & 92.00\% & 71.60\% \\
	5000 & 92.87\% & 65.00\% \\
	10000 & 94.73\% & 59.00\% \\
	15000 & 95.27\% & 54.20\% \\
	\hline
	\end{tabular}
	\caption{Accuracy of classifier with different amounts of features selected using frequency cut-off}
	\label{tab:frequency_cutoff}
\end{table}

\subsection{Mutual Information}

\textbf{Mutual information} is a numerical measure of the mutual dependence between two variables. In the context of text classification, this is the mutual dependence between a \textit{word existing in a document} and the \textit{document's class}. A word having a higher value for mutual information means it is more useful for determining whether or not a document is a particular class.

One way of selecting features would be to compute the amount of mutual information between each word and the "positive" movie review class, and select the $k$ words with the highest mutual information. Table \ref{tab:mutual_information} shows the results of running such a method on the movie review dataset. 

\begin{table}
	\centering
	\begin{tabular}{|l|l|l|}
	\hline
	\textbf{Number of Features $k$} & \textbf{Training Acc.} & \textbf{Test Acc.} \\
	\hline
	25 & 78.20\% & 69.40\% \\
	50 & 81.87\% & 75.20\% \\
	100 & 86.07\% & 79.80\% \\
	250 & 88.47\% & 85.60\% \\
	500 & 91.33\% & 87.40\% \\
	1000 & 92.20\% & 86.20\% \\
	2000 & 93.93\% & 85.40\% \\
	3000 & 94.60\% & 86.40\% \\
	5000 & 95.13\% & 82.20\% \\
	10000 & 95.13\% & 75.40\% \\
	15000 & 94.47\% & 73.00\% \\
	\hline
	\end{tabular}
	\caption{Accuracy of classifier with different amounts of features selected when ranking features based on mutual information}
	\label{tab:mutual_information}
\end{table}

Notice how the classification's accuracy on both the training and test datasets increases, and decreases, in a similar fashion to frequency cut-off as $k$ increases. The difference here is that all of the accuracies presented in this table are higher than than the corresponding accuracies in the frequency cutoff table. This indicates that mutual information is a \textbf{better approach to selecting features than frequency cutoff}, since the accuracies are higher regardless of the value used for $k$.

The reason for this is because words that are frequent in the training corpus may be frequent in all documents in said corpus, regardless of class. Therefore, some frequent words don't help distinguish between different document classes, as they're \textit{frequent in all documents}.

\subsection{Function Word Exclusion (Stopwords)}

There are many functional words which occur often in most English text. Since they frequently occur in most texts, they offer no semantic value when using a bag of words model. NLTK has a stoplist containing these functional words. I used this list to exclude functional words from all the documents and then trained/tested a binomial Naive Bayes classifier, just like the previous questions.

To determine whether or not excluding function words actually made a significant difference, I used both the frequency and mutual information feature selection methods, with and without function words. Table \ref{tab:function_word_exclusion}

\begin{table}
	\centering
	\begin{tabular}{|l|l|l|l|l|}
	\hline
	\begin{tabular}{c}
		\textbf{Function Word} \\
		\textbf{Exclusion}
	\end{tabular}
	 & \textbf{Feature Selection} & \textbf{Number of Features ($k$)}
	 & \textbf{Training Acc.} & \textbf{Test Acc.} \\
	\hline
	No & Frequency Cutoff & 3000 & 92.00\% & 71.60\% \\
	No & Mutual Information & 3000 & 94.60\% & 86.40\% \\
	Yes & Frequency Cutoff & 3000 &  90.37\% & 75.00\% \\
	Yes & Mutual Information & 3000 & 94.47\% & 86.40\% \\
	\hline
	\end{tabular}
	\caption{Accuracy of different feature selection methods with and without function words}
	\label{tab:function_word_exclusion}
\end{table}

As the results in the table show, removing function words barely affected the accuracy. Perhaps the reason this is the case when using frequency cutoff, even though function words will most likely be selected, there's still a relatively small number of function words being used as features compared to the large amount of other words used as features. In other words, it seems as if 

Even with function words still in the corpus, they don't affect mutual information feature selection either. This is because it is unlikely that they'll be selected because function words tend to have even distributions across all document classes (as they're used relatively evenly across the different document categories). Therefore, their mutual information score will be low and they won't be selected.

\subsection{Summary}

To summarise the findings in this section, preventing less frequent words becoming features using frequency cutoff did increase classification accuracy and generalisation, up to a point. Removing too many words reduced accuracy, but not removing enough meant that classifiers overfit their training dataset.

Selecting words with the highest amount of mutual information increased classification accuracy across the board, although you still had to be careful about how many features you kept.

Removing function words from consideration didn't have much of an effect , regardless of the feature selection method used. This is due to the way the words to use as features are selected.

\section{Using WordNet to Product Different Features}

WordNet is TODO. This section will describe different ways I used WordNet to extract new features from the dataset to use for classification, and discuss if they improved the accuracy or not.

\subsection{Synonyms and Hypernyms}

TODO: containsSynonymOf

TODO: containsHypernymOf

\begin{table}
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		$contains(x)$ Used &
		$containsSynonymOf(x)$ Used &
		$containsHypernym(x)$ Used &
		Training Acc. &
		Test Acc. \\
		\hline
		Yes & No & No & 90.73\% & 65.20\% \\
		No & Yes & No & 83.60\% & 70.40\% \\
		No & No & Yes & 71.93\% & 43.20\% \\
		Yes & Yes & No & 88.33\% & 75.40\% \\
		Yes & No & Yes & 84.33\% & 63.60\% \\
		Yes & Yes & Yes & 85.73\% & 68.40\% \\
		No & Yes & Yes & 81.13\% & 61.20\% \\
		\hline
	\end{tabular}
	\caption{Results of introducing synonym and hypernym features to movie review sentiment classification}
	\ref{tab:synonyms_and_hypernyms}
\end{table}

\subsection{Adjectives}

TODO

\subsection{Word Similiarity}

TODO: discuss using word similarity - state that doc could have containsSimilarWordTo(x) feature if word y is similar enough to x (> 0) using path similarity

TODO: mention computationally too slow to testout

\subsection{Feature Reduction using Similarity}

TODO: if using containsSimilarWordTo feature, if two of the selected words are similar above a certain threshold, REMOVE lower ranked one! This attempts to prevent redundancy and increase classification performance and accuracy (generlaisation) through a reduced set of features. 

\subsection{Summary}

TODO: not much 

\section{Theory: Bag of Words Representations}

\begin{itemize}
	\item \textbf{D1} -- there are differences between the two BOW representation, because commas and the word "London" appear more than once. \begin{itemize}
		\item \textbf{Bernoulli Model}: ['He', 'moved', 'from', 'London', ',', 'Ontario', 'to', 'England']
		\item \textbf{Multinomial Model}: ['He', 'moved', 'from', 'London', ',', 'Ontario'. ',', 'to', 'London', ',', 'England']
		\end{itemize}
	\item \textbf{D2} -- there are differences between the two BOW representation, because commas and the word "London" appear more than once. \begin{itemize}
		\item \textbf{Bernoulli Model}: ['He', 'moved', 'from', 'London', ',', 'England', 'to', 'Ontario']
		\item \textbf{Multinomial Model}: ['He', 'moved', 'from', 'London', ',', 'England', ',', 'to', 'London', ',', 'Ontario']
	  \end{itemize}
	\item \textbf{D3} -- there are no differences in the BOW representations, as there are no repeated tokens.
	\begin{itemize}
	\item \textbf{Bernoulli Model}: ['He', 'moved', 'from', 'England', 'to', 'London', ',', 'Ontario']
	\item \textbf{Multinomial Model}: ['He', 'moved', 'from', 'England', 'to', 'London', ',', 'Ontario']
	\end{itemize}
\end{itemize}

TODO: wanting frequency vectors or sets?

\section{Theory: Estimating Classifiers}

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|}
	\hline
	& \textbf{Document ID} & \textbf{Words in Document} & \textbf{in $c=$China?} \\
	\hline
	Training Set & 1 & Taipei Taiwan & yes \\
	& 2 & Macao Taiwan Shanghai & yes \\
	& 3 & Japan Sapporo & no \\
	& 4 & Sapporo Osaka Taiwan & no \\
	\hline
	Test Set & 5 & Taiwan Taiwan Sapporo & ? \\
	\hline 
	\end{tabular}
	\caption{Training and Test Datasets for Naive Bayes Classifiers}
	\label{tab:example_problem}
\end{table}

\begin{align}
P(China) = \frac{2}{4} \\
P(notChina) = \frac{2}{4} \\
V = \lbrace Taipei, Taiwan, Macao, Shanghai, Japan, Sapporo, Osaka \rbrace \\
|V| = 7
\end{align}

\subsection{Multinomial Naive Bayes Classifier}

\begin{tabular}{l}
\textbf{Large document} for class $China$ is: "Taipei Taiwan Macao Taiwan Shanghai". \\
\textbf{Large document} for class $notChina$ is: "Japan Sapporo Sapporo Osaka Taiwan" \\
$P(w_i|c) = \frac{n_i^c + 1}{n^c + |V|}$
\end{tabular}

\hspace{2pt}

For class $China$:
\begin{align}
n^{China} = 5 \\
P(Taipei|China) = P(Macao|China) = P(Shanghai|China) = \frac{1 + 1}{5 + 7} = \frac{2}{12} \\
P(Taiwan|China) = \frac{2 + 1}{5 + 7} = \frac{3}{12} \\
P(Japan|China) = P(Sapporo|China) = P(Osaka|China) = \frac{0 + 1}{5 + 7} = \frac{1}{12}
\end{align}

For class $notChina$:
\begin{align}
n^{notChina} = 5 \\
P(Japan|notChina) = P(Osaka|notChina) = P(Taiwan|notChina) = \frac{1 + 1}{5 + 7} = \frac{2}{12} \\
P(Sapporo|notChina) = \frac{2 + 1}{5 + 7} = \frac{3}{12} \\
P(Taipai|China) = P(Macao|China) = P(Shanghai|China) = \frac{0 + 1}{5 + 7} = \frac{1}{12} \\
\end{align}

Applying classifier to document 5:
\begin{align}
	P(China|doc5) & \propto P(China) \cdot \prod_{i \in positions} {P(w_i|China)} \\
	& \propto \frac{1}{2} \cdot P(Taiwan|China) \cdot P(Taiwan|China) \cdot P(Sapporo|China) \\
	& \propto \frac{1}{2} \cdot \frac{3}{12}^2 \cdot \frac{1}{12} \\
	& \propto \frac{6}{12} \cdot \frac{3}{12}^2 \cdot \frac{1}{12} \\
	& \propto \frac{54}{20736} \\
	& \propto 0.002604 \\
	& \nonumber \\ 
	P(notChina|doc5) & \propto P(notChina) \cdot \prod_{i \in positions} {P(w_i|China)} \\
	& \propto \frac{1}{2} \cdot P(Taiwan|notChina) \cdot P(Taiwan|notChina) \cdot P(Sapporo|notChina) \\
	& \propto \frac{1}{2} \cdot \frac{2}{12}^2 \cdot \frac{3}{12} \\
	& \propto \frac{6}{12} \cdot \frac{2}{12}^2 \cdot \frac{3}{12} \\	
	& \propto \frac{72}{20736} \\
	& \propto 0.00372
\end{align}

Therefore, a multinomial Naive Bayes classifier would label document 5 with the class $notChina$.

\subsection{Bernoulli Naive Bayes Classifier}

TODO: verify that this equation is correct
$P(e_i|c) = \frac{\#\;documents\;of\;class\;c\;that\;contain\;word w_i + 1}{\#\;documents\;of\;class\;c + 2}$

For class $China$:
\begin{align}
P(Taipei|China) = P(Macao|China) = P(Shanghai|China) = \frac{1 + 1}{2 + 2} = \frac{2}{4} \\
P(Taiwan|China) = \frac{2 + 1}{2 + 2} = \frac{3}{4} \\
P(Japan|China) = P(Sapporo|China) = P(Osaka|China) = \frac{0 + 1}{2 + 4} = \frac{1}{4}
\end{align}

For class $notChina$:
\begin{align}
P(Japan|China) = P(Osaka|China) = P(Taiwan|China) = \frac{1 + 1}{2 + 2} = \frac{2}{4} \\
P(Sapporo|China) = \frac{2 + 1}{2 + 2} = \frac{3}{4} \\
P(Taipei|China) = P(Macao|China) = P(Shanghai|China) = \frac{0 + 1}{2 + 4} = \frac{1}{4}
\end{align}

Applying classifier to document 5:
\begin{align}
	P(China|doc5) & \propto P(China) \cdot \prod_{w_i \in V} {Pe_i|China)} \\
	& \propto \frac{1}{2} \cdot P(Taiwan|China) \cdot P(Sapporo|China) \nonumber \\
	& \cdot (1 - P(Taipei|China)) \cdot (1 - P(Macao|China)) \cdot (1 - P(Shanghai|China)) \nonumber \\
	& \cdot (1 - P(Japan|China)) \cdot (1 - P(Osaka|China)) \\
	& \propto \frac{1}{2} \cdot \frac{3}{4} \cdot \frac{1}{4}
	\cdot \frac{2}{4} \cdot \frac{2}{4} \cdot \frac{2}{4}
	\cdot \frac{3}{4} \cdot \frac{3}{4} \\
	& \propto \frac{2}{4} \cdot \frac{3}{4} \cdot \frac{1}{4}
	\cdot \frac{2}{4} \cdot \frac{2}{4} \cdot \frac{2}{4}
	\cdot \frac{3}{4} \cdot \frac{3}{4} \\
	& \propto \frac{432}{65536} \\
	& \propto 0.006591 \\
	& \nonumber \\
	P(notChina|doc5) & \propto P(notChina) \cdot \prod_{w_i \in V} {Pe_i|China)} \\
	& \propto \frac{1}{2} \cdot P(Taiwan|notChina) \cdot P(Sapporo|notChina) \nonumber \\
	& \cdot (1 - P(Taipei|notChina)) \cdot (1 - P(Macao|notChina)) \nonumber \\
	& \cdot (1 - P(Shanghai|notChina)) \cdot (1 - P(Japan|notChina)) \nonumber \\
	& \cdot (1 - P(Osaka|notChina)) \\	
	& \propto \frac{1}{2} \cdot \frac{2}{4} \cdot \frac{3}{4} \cdot \frac{3}{4} \cdot \frac{3}{4} \cdot \frac{3}{4} \cdot \frac{2}{4} \cdot \frac{2}{4} \\
	& \propto \frac{2}{4} \cdot \frac{2}{4} \cdot \frac{3}{4} \cdot \frac{3}{4} \cdot \frac{3}{4} \cdot \frac{3}{4} \cdot \frac{2}{4} \cdot \frac{2}{4} \\
	& \propto \frac{1296}{64436} \\
	& \propto0.01977
\end{align}

Therefore, a Bernoulli, binomial Naive Bayes classifier would label document 5 with the class $notChina$.

\end{document}